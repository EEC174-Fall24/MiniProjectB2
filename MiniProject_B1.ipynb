{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **MiniProject B1: Medical Imaging Analysis**\n",
        "\n",
        "In this lab you will set up, and train a model that seeks to perform classification of medical OCT scans. In the data there are four classes of scans: Healthy, CNV, DME and DRUSEN. The last three are eye diseases that cause visible damage to the retina and can be spotted through the OCT scans."
      ],
      "metadata": {
        "id": "bH_fwVkSiVRY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvtViYA8iQed"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Overview**\n",
        "\n",
        "1. OCT Scan Dataset\n",
        "2. Model & Training\n",
        "3. Model interpretability\n",
        "4. Data Imbalance\n",
        "5. Unsupervised Learning (Clustering)"
      ],
      "metadata": {
        "id": "hkOsLbuWilK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. OCT Scan Dataset**"
      ],
      "metadata": {
        "id": "Fj76qLrzj7dk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optical Coherence Tomography (OCT) is a non-invasive imaging technique used to capture high-resolution cross-sectional images of the retina. In this dataset, there are four classes:\n",
        "\n",
        "* **Healthy (Normal):** Images of healthy retinas.\n",
        "\n",
        "\n",
        "* **CNV (Choroidal Neovascularization):** A condition often related to age-related macular degeneration, where abnormal blood vessels grow under the retina.\n",
        "\n",
        "\n",
        "* **DME (Diabetic Macular Edema):** Retinal swelling due to diabetes, potentially causing vision impairment.\n",
        "\n",
        "\n",
        "* **DRUSEN:** Deposits under the retina, often associated with age-related macular degeneration.\n",
        "\n",
        "\n",
        "\n",
        "Each class has its own folder in the dataset, and the dataset is split into training, validation, and test sets.\n",
        "\n",
        "`path_to_data:` This should be set to the shared drive path or the location where you have downloaded the OCT dataset."
      ],
      "metadata": {
        "id": "8be5ZVw7j6Gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [0, 1, 2, 3]\n",
        "class_labels = ['CNV', 'DME', 'DRUSEN', 'NORMAL']\n",
        "path_to_data = '/content/drive/Shareddrives/MiniProject-B1/OCT_Dataset/' # change accordingly"
      ],
      "metadata": {
        "id": "BKO7LBtAijBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.1 Data Augmentations**\n",
        "\n",
        "As you've seen previously, image augmentation can helps improve model generalization by creating additional, diverse images from the training data! For medical imaging this can be especially useful, as datasets are often small, and medical images can vary in quality due to differences in imaging devices or patient conditions.\n",
        "\n",
        "Based on your experience working with augmentations, apply transforms that will help your model train robustly:"
      ],
      "metadata": {
        "id": "fx3TExzVkUw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "gC6exOAfkafk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.2 Dataset and Loaders**\n",
        "\n",
        "Similar to previous labs, please make your dataloaders. In this lab we have all three: train, validation, and test."
      ],
      "metadata": {
        "id": "Ft-fmiCzkdo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batchsize = '''TODO'''\n",
        "\n",
        "#Initialize dataloader for train set\n",
        "dataset_train = '''TODO'''\n",
        "train_loader = '''TODO'''\n",
        "\n",
        "#Initialize dataloader for test set\n",
        "dataset_test = '''TODO'''\n",
        "test_loader = '''TODO'''\n",
        "\n",
        "#Initialize dataloader for val set\n",
        "dataset_val = '''TODO'''\n",
        "val_loader = '''TODO'''"
      ],
      "metadata": {
        "id": "QqZ7NINgkqik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.3 Display Images**\n",
        "\n",
        "Please complete `show_imgs()` to display five images per class, with the class names on top."
      ],
      "metadata": {
        "id": "QhhI6QL7kt63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_imgs('''TODO'''):\n",
        "    '''TODO'''\n",
        "\n",
        "show_imgs('''TODO''')"
      ],
      "metadata": {
        "id": "BClSPbuzkwXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Model & Training**"
      ],
      "metadata": {
        "id": "w_9TEgH2k2HQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1 Model Architecture**"
      ],
      "metadata": {
        "id": "f_nqSIeil0tA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By now, you've had experience selecting models based on your computer vision tasks and computational needs. Again, define your model architecture below. Adapt as necessary."
      ],
      "metadata": {
        "id": "nDRykKdsk7kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = '''TODO'''"
      ],
      "metadata": {
        "id": "SNs34vyYkyop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Training Utility functions\n",
        "\n",
        "**Training Function**\n",
        "- Define `train()`: Implement a loop to pass batches through the model, compute loss, backpropagate gradients, and update weights.\n",
        "\n",
        "**Testing Function**\n",
        "- Define `test_accuracy()`: Evaluate the model’s accuracy on given dataset.\n",
        "- Define `test_accuracy_per_class()`: Evaluate the model’s accuracy (per class) on given dataset\n",
        "\n",
        "**Learning Function**\n",
        "- Define `plot_learning_curves()`: Plot train and val losses and accuracies.\n",
        "\n",
        "**Plotting Confusion Matrix**\n",
        "- Define `plot_confusion_matrix()`: Plot the confusion matrix. Ensure it looks visually pleasing and is normalized.\n",
        "\n",
        "**Plotting ROC AUC Function**\n",
        "- Define `plot_roc_auc()`: Calculate the ROC AUC for each class (CNV, DME, DRUSEN, NORMAL) using the test set predictions.\n",
        "- You can use `roc_auc_score, roc_curve` from sklearn.\n",
        "\n",
        "```\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Only define the functions here. You will call and run them in Section 2.3 Training."
      ],
      "metadata": {
        "id": "RfD2RvpxlBya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train('''TODO'''):\n",
        "    '''TODO'''\n",
        "\n",
        "def test_accuracy('''TODO'''):\n",
        "    '''TODO'''\n",
        "\n",
        "def test_accuracy_per_class('''TODO'''):\n",
        "    '''TODO'''\n",
        "\n",
        "def plot_learning_curves('''TODO'''):\n",
        "    '''TODO'''\n",
        "\n",
        "def plot_confusion_matrix('''TODO'''):\n",
        "    '''TODO'''\n",
        "\n",
        "def plot_roc_auc('''TODO'''):\n",
        "    '''TODO'''"
      ],
      "metadata": {
        "id": "1N8YdQJalflF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.3 Training**\n",
        "\n",
        "Since we have a proper train, val, and test set; we will use train and val for training exclusively and test set only for testing and computing final accuracy.\n",
        "\n",
        "Choose all hyperparameters based on all your knowledge from previous labs and class. Your goal is to maximize accuracy. Adjust parameters based on **validation set only**."
      ],
      "metadata": {
        "id": "ImU28G6UlzJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "num_epochs = '''TODO'''\n",
        "lr = '''TODO'''\n",
        "# Add more if needed\n",
        "\n",
        "loss_function = '''TODO'''\n",
        "optimizer = '''TODO'''\n",
        "\n",
        "# train model\n",
        "train('''TODO''')"
      ],
      "metadata": {
        "id": "D85SCQsPma6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curves('''TODO''')"
      ],
      "metadata": {
        "id": "OU8rT9-kmusg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.4 Testing**\n",
        "\n",
        "Perform evaluation on the **test set**. Please report:\n",
        "\n",
        "\n",
        "*   Total Accuracy (across all classes)\n",
        "*   Accuracy per class\n",
        "*   Your AUC ROC curves\n",
        "*  Confusion Matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "c-PBrakKmyGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy('''TODO''')"
      ],
      "metadata": {
        "id": "VCyeVLRHmzWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy_per_class('''TODO''')"
      ],
      "metadata": {
        "id": "PQAVe0rOnfzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix('''TODO''')"
      ],
      "metadata": {
        "id": "i3ENJK-XnhT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_roc_auc('''TODO''')"
      ],
      "metadata": {
        "id": "8-RbkoHAn1yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Model Interpretability**\n",
        "\n",
        "For this assignment, you will interpret the model's results through the use of saliency mapping. You will use the following package: [GitHub](https://github.com/jacobgil/pytorch-grad-cam).\n",
        "\n",
        "Please go through the GitHub link above to learn its application. Below is an example code to help you get started:\n",
        "\n",
        "```python\n",
        "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "model = resnet50(pretrained=True) # Sample with ResNet50\n",
        "target_layers = [model.layer4[-1]]\n",
        "input_tensor = # Your input data\n",
        "# Note: input_tensor can be a batch tensor with several images!\n",
        "\n",
        "# Construct the CAM object once, and then re-use it on many images:\n",
        "cam = GradCAM(model=model, target_layers=target_layers, use_cuda=False)\n",
        "targets = ### your label\n",
        "\n",
        "# You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
        "\n",
        "# In this example grayscale_cam has only one image in the batch:\n",
        "grayscale_cam = grayscale_cam[0, :]\n",
        "visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
        "```\n",
        "\n",
        "Please install the package by running the command below."
      ],
      "metadata": {
        "id": "dvPKbGAIoM7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install git+https://github.com/jacobgil/pytorch-grad-cam.git"
      ],
      "metadata": {
        "id": "i4L5YRWauqsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.1 GRAD-CAM**\n",
        "\n",
        "Please write a function `OCT_grad_cam()` which takes in your trained model, and a given class label (CNV, DME, DRUSEN, NORMAL). You must display a randomly selected image from the given class, and then print the class your model predicted it as with its confidence, along with the original image and next to it the saliency map from GRAD-CAM. Please do this for all 4 classes. Examples will be provided by TAs during walkthrough."
      ],
      "metadata": {
        "id": "wlvQlydIuzfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def OCT_grad_cam('''TODO'''):\n",
        "    '''TODO'''"
      ],
      "metadata": {
        "id": "PDQuYilOp1Sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize for CNV"
      ],
      "metadata": {
        "id": "e5ku1yIArjf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize for DME"
      ],
      "metadata": {
        "id": "ECP-2ZWfrjXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize for DRUSEN"
      ],
      "metadata": {
        "id": "VAVsLfQ5rjOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize for NORMAL"
      ],
      "metadata": {
        "id": "1Lc8bb8arjBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Data Imbalance**\n",
        "\n",
        "## **4.1 Imbalanced Training Dataset**\n",
        "\n",
        "We have also provided in the dataset, a train_imbalanced set in your data folder. This is the same setup as others, only some samples are removed to make the dataset imbalanced. Please create a dataloader for this dataset -- `train_imbalanced_loader` and report the number of samples per class for the orignal `train_loader` and now the new `train_imbalanced_loader`."
      ],
      "metadata": {
        "id": "J3KAWkL5n5Lf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_imbalanced = '''TODO'''\n",
        "train_imbalanced _loader = '''TODO'''"
      ],
      "metadata": {
        "id": "Q_6nuPZKoTNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Samples per class for train_loader\n",
        "'''TODO'''"
      ],
      "metadata": {
        "id": "EVMfEuk_spFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Samples per class for train_imbalanced_loader\n",
        "'''TODO'''"
      ],
      "metadata": {
        "id": "iPOd0Vnlss2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.2 Training on Imbalanced Data**\n",
        "\n",
        "Please first replicate Section 3. Train model on the imbalanced dataset instead."
      ],
      "metadata": {
        "id": "ZaibGuACsT5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "model = '''TODO'''\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = '''TODO'''\n",
        "lr = '''TODO'''\n",
        "# Add more if needed\n",
        "\n",
        "loss_function = '''TODO'''\n",
        "optimizer = '''TODO'''\n",
        "\n",
        "# train model\n",
        "train('''TODO''')"
      ],
      "metadata": {
        "id": "cduUjegPsUap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curves('''TODO''')"
      ],
      "metadata": {
        "id": "aJy8sZdUtNrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy('''TODO''')"
      ],
      "metadata": {
        "id": "fSN1KL6huNMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.3 Weighted Sampler**\n",
        "\n",
        "Now, we will try to improve performance on the imbalanced dataset. We will use PyTorch's WeightedRandomSampler. You can find documentation [here](https://pytorch.org/docs/stable/data.html). Please create a weighted sampler, and train the same model (re-initialized) with exactly same traiing regiment as 4.2 for direct comparison."
      ],
      "metadata": {
        "id": "u7a214ohtRIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "# Calculate weights for each class\n",
        "'''TODO'''\n",
        "\n",
        "# Create a WeightedRandomSampler\n",
        "sampler = WeightedRandomSampler('''TODO''')\n",
        "\n",
        "# Model\n",
        "model = '''TODO'''\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = '''TODO'''\n",
        "lr = '''TODO'''\n",
        "# Add more if needed\n",
        "\n",
        "loss_function = '''TODO'''\n",
        "optimizer = '''TODO'''\n",
        "\n",
        "# train model\n",
        "train('''TODO''')"
      ],
      "metadata": {
        "id": "VaS83vBJtoqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curves('''TODO''')"
      ],
      "metadata": {
        "id": "51-k5JL5uWHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy('''TODO''')"
      ],
      "metadata": {
        "id": "J13tLVvJuSJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Unsupervised Learning (Clustering)**\n",
        "\n",
        "For this section, we will only use the **test set**.\n",
        "\n"
      ],
      "metadata": {
        "id": "yENCcn7ft1u9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.1 KMeans Clustering**\n",
        "\n",
        "\n",
        "Please implement KMeans Clustering on the OCT test set. You cannot use `sklearn.cluster.KMeans`, you must implement this algorithm on your own. Provided below is the function `kmeans()` and its definiton. You will then run your kmeans on the OCT test set.\n",
        "\n",
        "Please use sklearn's `normalized_mutual_info_score()` to evaluate the clustering performance.\n",
        "\n",
        "*Note:* The NMI performance will be very poor!! Expect 0.01!!"
      ],
      "metadata": {
        "id": "9exvmS5twapw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kmeans(X, k, max_iters=100):\n",
        "    \"\"\"\n",
        "    Performs K-means clustering on the given data.\n",
        "\n",
        "    Args:\n",
        "        X (torch.Tensor): Data tensor (n_samples, n_features).\n",
        "        k (int): Number of clusters.\n",
        "        max_iters (int): Maximum number of iterations.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Cluster assignments (n_samples,).\n",
        "        torch.Tensor: Cluster centroids (k, n_features).\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "PuF_D_vSt0-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import normalized_mutual_info_score\n",
        "\n",
        "assignments, centroids = kmeans('''TODO''')\n",
        "nmi_score = normalized_mutual_info_score('''TODO''')\n",
        "print(f\"NMI Score: {nmi_score:.2f}\")"
      ],
      "metadata": {
        "id": "A_0pf9KgvJtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.2 Plotting Clustering Results**\n",
        "\n",
        "We want to visualize the clusters our Kmeans algorithm created against the ground truth clusters. Please write a function `plot_pca_clusters()` which plots our cluster assignments and also the ground truth ones (two seperate plots). For the Ground truth plot, please include a legend identifying classes.\n",
        "\n",
        "Please use sklearn's PCA to reduce dimentionality to 2 or 3 dimensions for your plotting."
      ],
      "metadata": {
        "id": "3Y0LhspNweZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def plot_pca_clusters('''TODO'''):\n",
        "    '''TODO'''\n",
        "\n",
        "plot_pca_clusters('''TODO''')\n"
      ],
      "metadata": {
        "id": "Dp5YcWpxwnjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.3 Plotting for MNIST**\n",
        "\n",
        "Since the OCT dataset cannot be clustered well, we will test your `kmeans()` and `plot_pca_clusters()` functions by clustering the MNIST dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "regC1OamBZOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert to tensor\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize data\n",
        "])\n",
        "\n",
        "mnist_testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "mnist_testloader = torch.utils.data.DataLoader(mnist_testset, batch_size=64,\n",
        "                                         shuffle=False, num_workers=8)\n",
        "\n",
        "# perform clustering using mnist_testloader or mnist_testset\n",
        "assignments, centroids = kmeans('''TODO''')\n",
        "nmi_score = normalized_mutual_info_score('''TODO''')\n",
        "# Expected NMI around 0.5\n",
        "print(f\"NMI Score: {nmi_score:.2f}\")"
      ],
      "metadata": {
        "id": "3t956ALhCBIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_pca_clusters('''TODO''')"
      ],
      "metadata": {
        "id": "Y6jEUVDfCPo1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}